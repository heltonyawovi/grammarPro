# -*- coding: utf-8 -*-
"""Copy of PDF_Query_LLM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/181BSOH6KF_1o2lFG8DQ6eJd2MZyiSBNt


Requirements

!pip install langchain
!pip install openai
!pip install PyPDF2
#!pip install faiss-cpu
#!pip install tiktoken

pip install langchain & pip install openai & pip install faiss-cpu & pip install tiktoken



brew install xquartz --cask
brew install poppler antiword unrtf tesseract swig
pip install textract
"""


# from PyPDF2 import PdfReader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import ElasticVectorSearch, Pinecone, Weaviate, FAISS

# Get your API keys from openai, you will need to create an account.
# Here is the link to get the keys: https://platform.openai.com/account/billing/overview
import os
import openai


# os.environ["OPENAI_API_KEY"] = "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"
os.environ["OPENAI_API_KEY"] = "YOUR_API_KEY_HERE"
openai.api_key = os.getenv("OPENAI_API_KEY")

# As undesireable as it might be, more often than not there is extremely useful information embedded
# in Word documents, PowerPoint presentations, PDFs, etc—so-called “dark data”—that would be valuable
# for further textual analysis and visualization
import textract


def llmFromFileContent(
    filePath: str, prompt: str, splitInputAndAppendOutputs: bool = False
):
    raw_text = textract.process(filePath, method="pdfminer", headers=None)

    # print(raw_text[:100])
    raw_text = raw_text.decode("utf-8")  # encode in utf8 string

    return llmFromTextContent(
        textContent=raw_text,
        prompt=prompt,
        splitInputAndAppendOutputs=splitInputAndAppendOutputs,
    )


def llmFromTextContent(
    textContent: str, prompt: str, splitInputAndAppendOutputs: bool = False
):
    result = ""
    # We need to split the text that we read into smaller chunks so that during information retreival we don't hit the token size limits.

    text_splitter = CharacterTextSplitter(
        separator="\n",
        chunk_size=1500,
        chunk_overlap=200,
        length_function=len,
    )
    texts = text_splitter.split_text(textContent)

    # return (result, texts) # Comment later

    # len(texts)

    # texts[0]

    # texts[1]

    # Download embeddings from OpenAI
    embeddings = OpenAIEmbeddings()

    if splitInputAndAppendOutputs:
        for text in texts:
            print(len(texts))
            print("\n")
            print(len(text))
            print("\n")
            print(text)
            print("\n")
            # result += text

            response = openai.Completion.create(
                model="text-davinci-003",
                prompt=prompt + text,
                temperature=0.7,
                max_tokens=2000,
                top_p=1,
                frequency_penalty=0,
                presence_penalty=0,
            )
            print(response)
            print("\n")
            result += response.choices[0].text
    else:
        docsearch = FAISS.from_texts(texts, embeddings)

        # print(textContent)
        # print(prompt)
        # docsearch

        # from langchain.chains import load_qa_chain

        from langchain.chains.question_answering import load_qa_chain
        from langchain.llms import OpenAI

        chain = load_qa_chain(OpenAI(), chain_type="stuff")

        # query = "who are the authors of the article?"
        query = prompt
        docs = docsearch.similarity_search(query)
        result = chain.run(input_documents=docs, question=query)

    print(result)

    return (result, texts)
